{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1070,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import average_precision_score as auprc\n",
    "from sklearn.metrics import roc_auc_score as auc_score\n",
    "import keras\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers import Input, Dense, GRU, Lambda, Permute\n",
    "from keras.models import Model\n",
    "from interpolation_layer import single_channel_interp, cross_channel_interp\n",
    "import warnings\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ADMISSION_TYPE</th>\n",
       "      <th>LOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>165315</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>27.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>152223</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>131.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>124321</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>162.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>161859</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>68.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>129635</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>84.816667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID ADMISSION_TYPE         LOS\n",
       "0   165315      EMERGENCY   27.466667\n",
       "1   152223       ELECTIVE  131.916667\n",
       "2   124321      EMERGENCY  162.433333\n",
       "3   161859      EMERGENCY   68.566667\n",
       "4   129635      EMERGENCY   84.816667"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admission = pd.read_csv('Dataset/temp_admission.csv')\n",
    "admission = admission[[\"HADM_ID\", \"ADMISSION_TYPE\", \"LOS\"]]\n",
    "admission['LOS'] = admission['LOS'] * 24\n",
    "admission = admission[admission['LOS'] >= hours_looks_ahead]\n",
    "\n",
    "admission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_ = item220045_.loc[item220045_['HADM_ID'] == 152223].sort_values(by=['CHARTTIME'])\n",
    "# test1 = test_.groupby('HADM_ID')['CHARTTIME'].apply(list).tolist()[0]\n",
    "# test2 = test_.groupby('HADM_ID')['VALUENUM'].apply(list)\n",
    "# # for i in test1:\n",
    "# #     print(i)\n",
    "# # for j in test2:\n",
    "# #     print(j)\n",
    "# test1,test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3548.7631926656572"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# heart rate\n",
    "item220045 = pd.read_csv('Dataset/temp220045.csv')\n",
    "item220045.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item220045['CHARTTIME'] = pd.to_datetime(item220045['CHARTTIME']).dt.tz_localize(None)\n",
    "item220045 = item220045[item220045['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arterial Blood Pressure systolic\n",
    "item220050 = pd.read_csv('Dataset/temp220050.csv')\n",
    "item220050.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'ITEM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item220050 = item220050[item220050['VALUENUM'].notna()]\n",
    "item220050['CHARTTIME'] = pd.to_datetime(item220050['CHARTTIME']).dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Invasive Blood Pressure systolic\n",
    "item220179 = pd.read_csv('Dataset/temp220179.csv')\n",
    "item220179.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item220179 = item220179[item220179['VALUENUM'].notna()]\n",
    "item220179['CHARTTIME'] = pd.to_datetime(item220179['CHARTTIME']).dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature Fahrenheit\n",
    "item223761 = pd.read_csv('Dataset/temp223761.csv')\n",
    "item223761.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item223761 = item223761[item223761['VALUENUM'].notna()]\n",
    "item223761['CHARTTIME'] = pd.to_datetime(item223761['CHARTTIME']).dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1151,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature C\n",
    "item676 = pd.read_csv('Dataset/temp676.csv')\n",
    "item676['CHARTTIME'] = pd.to_datetime(item676['CHARTTIME']).dt.tz_localize(None)\n",
    "item676 = item676[item676['VALUENUM'].notna()]\n",
    "item676['VALUENUME'] = item676['VALUENUM'] * 9/5.0 + 32 # convert to F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired O2 Fraction (FiO2)\n",
    "item223835 = pd.read_csv('Dataset/temp223835.csv')\n",
    "item223835.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'ITEM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item223835 = item223835[item223835['VALUENUM'].notna()]\n",
    "item223835['CHARTTIME'] = pd.to_datetime(item223835['CHARTTIME']).dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RR\n",
    "item618 = pd.read_csv('Dataset/temp618.csv')\n",
    "item618['CHARTTIME'] = pd.to_datetime(item618['CHARTTIME']).dt.tz_localize(None)\n",
    "item618 = item618[item618['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RR should be merged with 618\n",
    "item220210 = pd.read_csv('Dataset/temp220210.csv')\n",
    "item220210['CHARTTIME'] = pd.to_datetime(item220210['CHARTTIME']).dt.tz_localize(None)\n",
    "item220210 = item220210[item220210['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spo2 \n",
    "item646 = pd.read_csv('Dataset/temp646.csv')\n",
    "item646['CHARTTIME'] = pd.to_datetime(item646['CHARTTIME']).dt.tz_localize(None)\n",
    "item646 = item646[item646['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spo2 should be merged with 646\n",
    "item220277 = pd.read_csv('Dataset/temp220277.csv')\n",
    "item220277['CHARTTIME'] = pd.to_datetime(item220277['CHARTTIME']).dt.tz_localize(None)\n",
    "item220277 = item220277[item220277['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_spo2 = item646.append(item220277, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBP\n",
    "item8441 = pd.read_csv('Dataset/temp8441.csv')\n",
    "item8441['CHARTTIME'] = pd.to_datetime(item8441['CHARTTIME']).dt.tz_localize(None)\n",
    "item8441 = item8441[item8441['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50882 serum bicarbon- ate level\n",
    "item50882 = pd.read_csv('Dataset_bench/temp50882.csv')\n",
    "item50882.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item50882['CHARTTIME'] = pd.to_datetime(item50882['CHARTTIME']).dt.tz_localize(None)\n",
    "item50882 = item50882[item50882['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50971 potassium level\n",
    "item50971 = pd.read_csv('Dataset_bench/temp50971.csv')\n",
    "item50971.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item50971['CHARTTIME'] = pd.to_datetime(item50971['CHARTTIME']).dt.tz_localize(None)\n",
    "item50971 = item50971[item50971['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50885 bilirubin level\n",
    "item50885 = pd.read_csv('Dataset_bench/temp50885.csv')\n",
    "item50885.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item50885['CHARTTIME'] = pd.to_datetime(item50885['CHARTTIME']).dt.tz_localize(None)\n",
    "item50885 = item50885[item50885['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50983 sodium level\n",
    "item50983 = pd.read_csv('Dataset_bench/temp50983.csv')\n",
    "item50983.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item50983['CHARTTIME'] = pd.to_datetime(item50983['CHARTTIME']).dt.tz_localize(None)\n",
    "item50983 = item50983[item50983['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 51300 white blood cells count\n",
    "item51300 = pd.read_csv('Dataset_bench/temp51300.csv')\n",
    "item51300.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item51300['CHARTTIME'] = pd.to_datetime(item51300['CHARTTIME']).dt.tz_localize(None)\n",
    "item51300 = item51300[item51300['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 220739 glasgow coma scale\n",
    "item220739 = pd.read_csv('Dataset_bench/temp220739.csv')\n",
    "item220739.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item220739['CHARTTIME'] = pd.to_datetime(item220739['CHARTTIME']).dt.tz_localize(None)\n",
    "item220739 = item220739[item220739['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item 50822\n",
    "item50822 = pd.read_csv('Dataset_bench/temp50822.csv')\n",
    "item50822.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item50822['CHARTTIME'] = pd.to_datetime(item50822['CHARTTIME']).dt.tz_localize(None)\n",
    "item50822 = item50822[item50822['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 941,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item 50821\n",
    "item50821 = pd.read_csv('Dataset_bench/temp50821.csv')\n",
    "item50821.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item50821['CHARTTIME'] = pd.to_datetime(item50821['CHARTTIME']).dt.tz_localize(None)\n",
    "item50821 = item50821[item50821['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urine output 226559 , 226560, 226561, 226563, 226564, 226565, 226567, 226584\n",
    "\n",
    "item226559 = pd.read_csv('Dataset_bench/temp226559.csv')\n",
    "item226559.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item226559['CHARTTIME'] = pd.to_datetime(item226559['CHARTTIME']).dt.tz_localize(None)\n",
    "item226559 = item226559[item226559['VALUENUM'].notna()]\n",
    "\n",
    "item226560 = pd.read_csv('Dataset_bench/temp226560.csv')\n",
    "item226560.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item226560['CHARTTIME'] = pd.to_datetime(item226560['CHARTTIME']).dt.tz_localize(None)\n",
    "item226560 = item226560[item226560['VALUENUM'].notna()]\n",
    "\n",
    "item226561 = pd.read_csv('Dataset_bench/temp226561.csv')\n",
    "item226561.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item226561['CHARTTIME'] = pd.to_datetime(item226561['CHARTTIME']).dt.tz_localize(None)\n",
    "item226561 = item226561[item226561['VALUENUM'].notna()]\n",
    "\n",
    "item226563 = pd.read_csv('Dataset_bench/temp226563.csv')\n",
    "item226563.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item226563['CHARTTIME'] = pd.to_datetime(item226563['CHARTTIME']).dt.tz_localize(None)\n",
    "item226563 = item226563[item226563['VALUENUM'].notna()]\n",
    "\n",
    "item226564 = pd.read_csv('Dataset_bench/temp226564.csv')\n",
    "item226564.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item226564['CHARTTIME'] = pd.to_datetime(item226564['CHARTTIME']).dt.tz_localize(None)\n",
    "item226564 = item226564[item226564['VALUENUM'].notna()]\n",
    "\n",
    "item226565 = pd.read_csv('Dataset_bench/temp226565.csv')\n",
    "item226565.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item226565['CHARTTIME'] = pd.to_datetime(item226565['CHARTTIME']).dt.tz_localize(None)\n",
    "item226565 = item226565[item226565['VALUENUM'].notna()]\n",
    "\n",
    "item226567 = pd.read_csv('Dataset_bench/temp226567.csv')\n",
    "item226567.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item226567['CHARTTIME'] = pd.to_datetime(item226567['CHARTTIME']).dt.tz_localize(None)\n",
    "item226567 = item226567[item226567['VALUENUM'].notna()]\n",
    "\n",
    "item226584 = pd.read_csv('Dataset_bench/temp226584.csv')\n",
    "item226584.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item226584['CHARTTIME'] = pd.to_datetime(item226584['CHARTTIME']).dt.tz_localize(None)\n",
    "item226584 = item226584[item226584['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O2 50816\n",
    "item50816 = pd.read_csv('Dataset_bench/temp50816.csv')\n",
    "item50816.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item50816['CHARTTIME'] = pd.to_datetime(item50816['CHARTTIME']).dt.tz_localize(None)\n",
    "item50816 = item50816[item50816['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 52 \n",
    "item52 = pd.read_csv('Dataset_bench/temp52.csv')\n",
    "item52['CHARTTIME'] = pd.to_datetime(item52['CHARTTIME']).dt.tz_localize(None)\n",
    "item52 = item52[item52['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 959,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 113\n",
    "item113 = pd.read_csv('Dataset_bench/temp113.csv')\n",
    "item113['CHARTTIME'] = pd.to_datetime(item113['CHARTTIME']).dt.tz_localize(None)\n",
    "item113 = item113[item113['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 995,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50902\n",
    "item50902 = pd.read_csv('Dataset_bench/temp50902.csv')\n",
    "item50902['CHARTTIME'] = pd.to_datetime(item50902['CHARTTIME']).dt.tz_localize(None)\n",
    "item50902 = item50902[item50902['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1020,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 834\n",
    "item834 = pd.read_csv('Dataset_bench/temp834.csv')\n",
    "item834['CHARTTIME'] = pd.to_datetime(item834['CHARTTIME']).dt.tz_localize(None)\n",
    "item834 = item834[item834['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1021,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge 50822 and 50971 potassium level\n",
    "item_po = item50822.append(item50971, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge rr\n",
    "item_rr = item618.append(item220210, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge uo\n",
    "item_uo = item226559.append(item226560, ignore_index=True)\n",
    "item_uo = item_uo.append(item226561, ignore_index=True)\n",
    "item_uo = item_uo.append(item226563, ignore_index=True)\n",
    "item_uo = item_uo.append(item226564, ignore_index=True)\n",
    "item_uo = item_uo.append(item226565, ignore_index=True)\n",
    "item_uo = item_uo.append(item226567, ignore_index=True)\n",
    "item_uo = item_uo.append(item226584, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1150,
   "metadata": {},
   "outputs": [],
   "source": [
    "# systolic blood pressure\n",
    "item_sbp = item220179.append(item220050, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# body tempera- ture\n",
    "item_temp = item223761.append(item676, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pao2 / fio2 ratio\n",
    "item_pf = item50821.append(item50816, ignore_index=True)\n",
    "item_pf = item_pf.append(item223835, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1166,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_dict = {}\n",
    "np.random.seed(10)\n",
    "num_features = 10\n",
    "max_length = 2000 # length of timestamp\n",
    "gpu_num = 0\n",
    "ref_points = 192\n",
    "hid = 100\n",
    "batch = 128\n",
    "epoch = 20\n",
    "num_ids = 200\n",
    "hours_looks_ahead = 24\n",
    "num_fold = 3\n",
    "feature_set = 2\n",
    "stds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1167,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return two lists of hadm_id: list of timestamps (ascending order), list of corresponding vals of the item\n",
    "def time_val_toLst(df, hadmin_id):\n",
    "    temp = df.loc[df['HADM_ID'] == hadmin_id].sort_values(by=['CHARTTIME'])\n",
    "    if not temp.empty: \n",
    "        times = temp.groupby('HADM_ID')['CHARTTIME'].apply(list).tolist()[0]\n",
    "        vals = temp.groupby('HADM_ID')['VALUENUM'].apply(list).tolist()[0]\n",
    "        return [times, vals]\n",
    "    else:\n",
    "        return [] # if id does not have this item, return empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1197,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(df, los=hours_looks_ahead):\n",
    "    a = np.full((len(df), num_features, max_length), -100) # initlizae all as missing\n",
    "    timestamps = []\n",
    "    for i in range(len(df)):\n",
    "        l = []\n",
    "    \n",
    "        # find all the unique observed timestamps\n",
    "        for j in range(num_features):\n",
    "            ts_ij = df[i][j]\n",
    "            if ts_ij != []:\n",
    "                for ts in df[i][j][0]:\n",
    "                    if ts not in l:\n",
    "                        l.append(ts)\n",
    "        l.sort()\n",
    "        T = copy.deepcopy(l)\n",
    "        TS = []\n",
    "        for t in T:\n",
    "            if (t - T[0]).total_seconds() / 3600 <= los:\n",
    "                TS.append(t)\n",
    "        timestamps.append(TS)\n",
    "        for j in range(num_features):\n",
    "            s_ij = df[i][j]\n",
    "            if s_ij != []:\n",
    "                ts_ij = s_ij[0] \n",
    "                c_max = len(ts_ij)\n",
    "                c = 0\n",
    "                for k in range(len(TS)):\n",
    "                    if c < c_max:\n",
    "                        ts_ijc = ts_ij[c] # cur ts\n",
    "                        diff = abs(TS[k] - ts_ijc).seconds / 60 # difference between ts's\n",
    "                        if TS[k] == ts_ijc or diff < 5:\n",
    "                            a[i,j,k] = s_ij[1][c] \n",
    "                            c += 1\n",
    "    return a, timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1198,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_format(x, T):\n",
    "    real_len = 200\n",
    "    \n",
    "    for i in range(len(T)):\n",
    "        if len(T[i]) > real_len:\n",
    "            T[i] = T[i][:real_len]\n",
    "\n",
    "    x = a[:, :, :real_len]\n",
    "    M = np.zeros_like(x)\n",
    "    delta = np.zeros_like(x)\n",
    "\n",
    "    for t in T:\n",
    "        for i in range(1, len(t)):\n",
    "            t[i] = (t[i] - t[0]).total_seconds() / 3600 # hours difference\n",
    "        if len(t) != 0:\n",
    "            t[0] = 0\n",
    "    \n",
    "    # count outliers and negative values as missing values\n",
    "    # M = 0 indicates missing value\n",
    "    # M = 1 indicates observed value\n",
    "    # now since we have mask variable, we don't need -100\n",
    "    M[x > 500] = 0\n",
    "    x[x > 500] = 0.0\n",
    "    M[x < 0] = 0\n",
    "    x[x < 0] = 0.0\n",
    "    M[x > 0] = 1\n",
    "\n",
    "    for i in range(num_features):\n",
    "        for j in range(x.shape[0]):\n",
    "            for k in range(len(T[j])):\n",
    "                delta[j, i, k] = T[j][k]\n",
    "    return x, M, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1199,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_mean(M, x):\n",
    "    counts = np.sum(np.sum(M, axis=2), axis=0)\n",
    "    mean_values = np.sum(np.sum(x*M, axis=2), axis=0)/counts\n",
    "    for i in range(M.shape[0]):\n",
    "        for j in range(M.shape[1]):\n",
    "            if np.sum(M[i, j]) == 0:\n",
    "                M[i, j, 0] = 1\n",
    "                x[i, j, 0] = mean_values[j]\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1200,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hold_out(mask, perc=0.2):\n",
    "    \"\"\"To implement the autoencoder component of the loss, we introduce a set\n",
    "    of masking variables mr (and mr1) for each data point. If drop_mask = 0,\n",
    "    then we removecthe data point as an input to the interpolation network,\n",
    "    and includecthe predicted value at this time point when assessing\n",
    "    the autoencoder loss. In practice, we randomly select 20% of the\n",
    "    observed data points to hold out from\n",
    "    every input time series.\"\"\"\n",
    "    drop_mask = np.ones_like(mask)\n",
    "    drop_mask *= mask\n",
    "    for i in range(mask.shape[0]):\n",
    "        for j in range(mask.shape[1]):\n",
    "            count = np.sum(mask[i, j], dtype='int')\n",
    "            if int(0.20*count) > 1:\n",
    "                index = 0\n",
    "                r = np.ones((count, 1))\n",
    "                b = np.random.choice(count, int(0.20*count), replace=False)\n",
    "                r[b] = 0\n",
    "                for k in range(mask.shape[2]):\n",
    "                    if mask[i, j, k] > 0:\n",
    "                        drop_mask[i, j, k] = r[index]\n",
    "                        index += 1\n",
    "    return drop_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1181,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(123)\n",
    "adm_ids = admission['HADM_ID'].tolist()\n",
    "needed_ids = random.sample(adm_ids,  num_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1190,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vitals dictionary: {id: [item1, item2, item3, item4, item5]}\n",
    "# item1 = [list of timestamps, list of vals]\n",
    "# only use the first 200 hadm_ids as sample\n",
    "\n",
    "# 220045, spo2, rr,  \n",
    "# needed_ids = adm_ids[:num_ids]\n",
    "for adm_id in needed_ids:\n",
    "    if feature_set == 1:\n",
    "        vitals_dict[adm_id] = [time_val_toLst(item220045, adm_id)]\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item220050, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item220179, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item676, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item223835, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item_rr, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item_spo2, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item8441, adm_id))\n",
    "        \n",
    "    if feature_set == 2:\n",
    "        vitals_dict[adm_id] = [time_val_toLst(item220739, adm_id)]\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item_sbp, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item220045, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item_temp, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item_pf, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item_uo, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item51300, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item50882, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item50983, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item_po, adm_id))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1191,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = []\n",
    "if feature_set == 1:\n",
    "    stds.append(item220045['VALUENUM'].std())\n",
    "    stds.append(item220050['VALUENUM'].std())\n",
    "    stds.append(item220179['VALUENUM'].std())\n",
    "    stds.append(item676['VALUENUM'].std())\n",
    "    stds.append(item223835['VALUENUM'].std())\n",
    "    stds.append(item_rr['VALUENUM'].std())\n",
    "    stds.append(item_spo2['VALUENUM'].std())\n",
    "    stds.append(item8441['VALUENUM'].std())\n",
    "if feature_set == 2:\n",
    "    stds.append(item220739['VALUENUM'].std())\n",
    "    stds.append(item_sbp['VALUENUM'].std())\n",
    "    stds.append(item220045['VALUENUM'].std())\n",
    "    stds.append(item_temp['VALUENUM'].std())\n",
    "    stds.append(item_pf['VALUENUM'].std())\n",
    "    stds.append(item_uo['VALUENUM'].std())\n",
    "    stds.append(item51300['VALUENUM'].std())\n",
    "    stds.append(item50882['VALUENUM'].std())\n",
    "    stds.append(item50983['VALUENUM'].std())\n",
    "    stds.append(item_po['VALUENUM'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1192,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def y_trans(val):\n",
    "#     if val <= 48:\n",
    "#         return 0\n",
    "#     elif 48 < val <= 168:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1193,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  1.05972222,  16.29634623,  78.05850694, 129.96041667])]\n"
     ]
    }
   ],
   "source": [
    "vitals = [vitals_dict[x] for x in needed_ids] # hadm_id(los>=48h): all the vitals values\n",
    "kb = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans')\n",
    "label = kb.fit_transform(admission[admission['HADM_ID'].isin(needed_ids)].LOS.to_numpy().reshape(-1,1))\n",
    "# label = admission[admission['HADM_ID'].isin(needed_ids)].LOS.tolist()\n",
    "# label = [y_trans(l) for l in label]\n",
    "print(kb.bin_edges_ / 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 40, 200) (200, 1)\n"
     ]
    }
   ],
   "source": [
    "a, ts = flatten(vitals, hours_looks_ahead)\n",
    "x, m, T = input_format(a, ts)\n",
    "missing_mean(m, x)\n",
    "X = np.concatenate((x, m, T, hold_out(m)), axis=1)  # input format\n",
    "y = np.array(label)\n",
    "print(X.shape, y.shape)\n",
    "timestamp = X.shape[2]\n",
    "num_features = X.shape[1] // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 1202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1203,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [17.3, 22, 22.45, 2.32, 23.41，5.7, 3.33, 14.1]\n",
    "def customloss(ytrue, ypred):\n",
    "    \"\"\" Autoencoder loss\n",
    "    \"\"\"\n",
    "    num_fs = num_features\n",
    "    # standard deviation of each feature mentioned in paper for MIMIC_III data\n",
    "    wc = np.array(stds)\n",
    "    print(wc.shape, ytrue.shape)\n",
    "    wc.shape = (1, num_fs)\n",
    "    y = ytrue[:, :num_fs, :]\n",
    "    m2 = ytrue[:, 3*num_fs:4*num_fs, :]\n",
    "    m2 = 1 - m2\n",
    "    m1 = ytrue[:, num_fs:2*num_fs, :]\n",
    "    m = m1*m2\n",
    "    ypred = ypred[:, :num_fs, :]\n",
    "    x = (y - ypred)*(y - ypred)\n",
    "    x = x*m\n",
    "    count = tf.reduce_sum(m, axis=2)\n",
    "    count = tf.where(count > 0, count, tf.ones_like(count))\n",
    "    x = tf.reduce_sum(x, axis=2)/count\n",
    "    x = x/(wc**2)  # dividing by standard deviation\n",
    "    x = tf.reduce_sum(x, axis=1)/num_fs\n",
    "    return tf.reduce_mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1204,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_net():\n",
    "    num_fs = num_features\n",
    "    if gpu_num > 1:\n",
    "        dev = \"/cpu:0\"\n",
    "    else:\n",
    "        dev = \"/gpu:0\"\n",
    "    with tf.device(dev):\n",
    "        main_input = Input(shape=(4*num_fs, timestamp), name='input')\n",
    "        sci = single_channel_interp(ref_points, hours_looks_ahead)\n",
    "        cci = cross_channel_interp()\n",
    "        interp = cci(sci(main_input))\n",
    "        reconst = cci(sci(main_input, reconstruction=True),\n",
    "                      reconstruction=True)\n",
    "        aux_output = Lambda(lambda x: x, name='aux_output')(reconst)\n",
    "        z = Permute((2, 1))(interp)\n",
    "        z = GRU(hid, activation='tanh', recurrent_dropout=0.2, dropout=0.2)(z)\n",
    "        main_output = Dense(1, activation='sigmoid', name='main_output')(z)\n",
    "        orig_model = Model([main_input], [main_output, aux_output])\n",
    "    if gpu_num > 1:\n",
    "        model = multi_gpu_model(orig_model, gpus=gpu_num)\n",
    "    else:\n",
    "        model = orig_model\n",
    "    print(orig_model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1205,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "results = {}\n",
    "results['loss'] = []\n",
    "results['auc'] = []\n",
    "results['acc'] = []\n",
    "results['auprc'] = []\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0000, patience=20, verbose=0)\n",
    "callbacks_list = [earlystop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold: 1\n",
      "Model: \"model_116\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 40, 200)      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "single_channel_interp_116 (sing multiple             10          input[0][0]                      \n",
      "                                                                 input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "cross_channel_interp_116 (cross multiple             100         single_channel_interp_116[0][0]  \n",
      "                                                                 single_channel_interp_116[1][0]  \n",
      "__________________________________________________________________________________________________\n",
      "permute_116 (Permute)           (None, 192, 30)      0           cross_channel_interp_116[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "gru_116 (GRU)                   (None, 100)          39300       permute_116[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 1)            101         gru_116[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "aux_output (Lambda)             (None, 10, 200)      0           cross_channel_interp_116[1][0]   \n",
      "==================================================================================================\n",
      "Total params: 39,511\n",
      "Trainable params: 39,511\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "(10,) (?, ?, ?)\n",
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      " - 30s - loss: 18016876705611776.0000 - main_output_loss: 18198865207361536.0000 - aux_output_loss: 0.0382 - main_output_accuracy: 0.4625 - val_loss: 0.6635 - val_main_output_loss: 0.6618 - val_aux_output_loss: 0.0084 - val_main_output_accuracy: 0.6500\n",
      "Epoch 2/20\n",
      " - 5s - loss: 27251571788087296.0000 - main_output_loss: 27526840537055232.0000 - aux_output_loss: 0.0382 - main_output_accuracy: 0.4875 - val_loss: 0.6513 - val_main_output_loss: 0.6496 - val_aux_output_loss: 0.0083 - val_main_output_accuracy: 0.7000\n",
      "Epoch 3/20\n",
      " - 4s - loss: 15611949496664064.0000 - main_output_loss: 15769645663387648.0000 - aux_output_loss: 0.0382 - main_output_accuracy: 0.5500 - val_loss: 0.6424 - val_main_output_loss: 0.6404 - val_aux_output_loss: 0.0084 - val_main_output_accuracy: 0.7000\n",
      "Epoch 4/20\n",
      " - 3s - loss: 21113391359721472.0000 - main_output_loss: 21326657960804352.0000 - aux_output_loss: 0.0382 - main_output_accuracy: 0.5500 - val_loss: 0.6288 - val_main_output_loss: 0.6270 - val_aux_output_loss: 0.0081 - val_main_output_accuracy: 0.7000\n",
      "Epoch 5/20\n"
     ]
    }
   ],
   "source": [
    "# 5-fold cross-validation\n",
    "i = 0\n",
    "kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "for train, test in kfold.split(np.zeros(len(y)), y):\n",
    "    print(\"Running Fold:\", i+1)\n",
    "    model = interp_net()  # re-initializing every time\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={'main_output': 'binary_crossentropy', 'aux_output': customloss},\n",
    "        loss_weights={'main_output': 0.99, 'aux_output': 0.99},\n",
    "        metrics={'main_output': 'accuracy'})\n",
    "    model.fit(\n",
    "        {'input': X[train]}, {'main_output': y[train], 'aux_output': X[train]},\n",
    "        batch_size=batch,\n",
    "        callbacks=callbacks_list,\n",
    "        nb_epoch=epoch,\n",
    "        validation_split=0.20,\n",
    "        verbose=2)\n",
    "    y_pred = model.predict(X[test], batch_size=batch)\n",
    "    y_pred = y_pred[0]\n",
    "    total_loss, score, reconst_loss, acc = model.evaluate(\n",
    "        {'input': X[test]},\n",
    "        {'main_output': y[test], 'aux_output': X[test]},\n",
    "        batch_size=batch,\n",
    "        verbose=0)\n",
    "    results['loss'].append(score)\n",
    "    results['acc'].append(acc)\n",
    "#     results['auc'].append(auc_score(y[test], y_pred))\n",
    "#     y[test] = preprocessing.label_binarize(y_, classes=[0, 1, 2])\n",
    "    results['auprc'].append(auprc(y[test], y_pred))\n",
    "    print(results)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1148,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [1.2251596450805664, nan],\n",
       " 'auc': [],\n",
       " 'acc': [0.17000000178813934, 0.0],\n",
       " 'auprc': []}"
      ]
     },
     "execution_count": 1148,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.8490171432495117,\n",
       "  0.8478379249572754,\n",
       "  0.5991557240486145,\n",
       "  0.20699283480644226,\n",
       "  1.7641651630401611],\n",
       " 'auc': [0.44594594594594594,\n",
       "  0.4864864864864865,\n",
       "  0.5263157894736843,\n",
       "  0.5263157894736842,\n",
       "  0.5789473684210527],\n",
       " 'acc': [0.07500000298023224,\n",
       "  0.07500000298023224,\n",
       "  0.949999988079071,\n",
       "  0.949999988079071,\n",
       "  0.05000000074505806],\n",
       " 'auprc': [0.075,\n",
       "  0.07894736842105263,\n",
       "  0.05263157894736842,\n",
       "  0.05555555555555555,\n",
       "  0.058823529411764705]}"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'loss': [0.5623685717582703, 0.9635432362556458],\n",
    " 'auc': [],\n",
    " 'acc': [0.800000011920929, 0.17000000178813934],\n",
    " 'auprc': []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'loss': [0.7575289607048035, 0.30154579877853394, 0.14339998364448547, nan], 'auc': [], 'acc': [0.42500001192092896, 0.9750000238418579, 0.9750000238418579, 0.0], 'auprc': [0.06666666666666667, 0.3333333333333333, 0.09090909090909091]}\n",
    "# for lst in timestamps:\n",
    "#     if len(lst) > max_length:\n",
    "#         timestamps[i] = lst[:max_length]\n",
    "\n",
    "# x = a[:, :, :max_length]\n",
    "# M = np.zeros_like(x)\n",
    "# delta = np.zeros_like(x)\n",
    "\n",
    "# for t in timestamps:\n",
    "#     for i in range(1, len(t)):\n",
    "#         t[i] = (t[i] - t[0]).total_seconds() / 3600 # hours difference\n",
    "#     if len(t) != 0:\n",
    "#         t[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.full((len(vitals), num_features, max_length), -100) # initlizae all as missing\n",
    "# timestamps = []\n",
    "# for i in range(len(vitals)):\n",
    "#     l = []\n",
    "    \n",
    "#     # find all the unique observed timestamps\n",
    "#     for j in range(num_features):\n",
    "#         ts_ij = vitals[i][j]\n",
    "#         if ts_ij != []:\n",
    "#             for ts in vitals[i][j][0]:\n",
    "#                 if ts not in l:\n",
    "#                     l.append(ts)\n",
    "#     l.sort()\n",
    "#     T = copy.deepcopy(l)\n",
    "#     TS = []\n",
    "#     for t in T:\n",
    "#         if (t - T[0]).total_seconds() / 3600 <= 72:\n",
    "#             TS.append(t)\n",
    "#     timestamps.append(TS)\n",
    "#     for j in range(num_features):\n",
    "#         s_ij = vitals[i][j]\n",
    "#         if s_ij != []:\n",
    "#             ts_ij = s_ij[0] \n",
    "#             c_max = len(ts_ij)\n",
    "#             c = 0\n",
    "#             for k in range(len(TS)):\n",
    "#                 if c < c_max:\n",
    "#                     ts_ijc = ts_ij[c] # cur ts\n",
    "#                     diff = abs(TS[k] - ts_ijc).seconds / 60 # difference between ts's\n",
    "#                     if TS[k] == ts_ijc or diff < 5:\n",
    "#                         a[i,j,k] = s_ij[1][c] \n",
    "#                         c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # count outliers and negative values as missing values\n",
    "# # M = 0 indicates missing value\n",
    "# # M = 1 indicates observed value\n",
    "# # now since we have mask variable, we don't need -100\n",
    "# M[x > 500] = 0\n",
    "# x[x > 500] = 0.0\n",
    "# M[x < 0] = 0\n",
    "# x[x < 0] = 0.0\n",
    "# M[x > 0] = 1\n",
    "\n",
    "# for i in range(num_features):\n",
    "#         for j in range(x.shape[0]):\n",
    "#             for k in range(len(timestamps[j])):\n",
    "#                 delta[j, i, k] = timestamps[j][k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39, 38, 13, 33, 22, 2, 37, 21, 29, 32, 27, 31, 53, 35, 47, 30, 31, 44, 25, 28]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.concatenate((x, m, T, hold_out(m)), axis=1)  # input format\n",
    "y = np.array(label)\n",
    "print(x.shape, y.shape)\n",
    "timestamp = x.shape[2]\n",
    "num_features = x.shape[1] // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.18130822479724884, nan],\n",
       " 'auc': [0.6025641025641025],\n",
       " 'acc': [0.9750000238418579, 0.0],\n",
       " 'auprc': [0.04805491990846682]}"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
