{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 513,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import copy\n",
    "from datetime import datetime\n",
    "import argparse\n",
    "import logging, os\n",
    "logging.disable(logging.WARNING)\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"3\"\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import average_precision_score as auprc\n",
    "from sklearn.metrics import roc_auc_score as auc_score\n",
    "import keras\n",
    "from keras.utils import multi_gpu_model\n",
    "from keras.layers import Input, Dense, GRU, Lambda, Permute\n",
    "from keras.models import Model\n",
    "from interpolation_layer import single_channel_interp, cross_channel_interp\n",
    "import warnings\n",
    "from sklearn.preprocessing import KBinsDiscretizer\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 514,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>HADM_ID</th>\n",
       "      <th>ADMISSION_TYPE</th>\n",
       "      <th>LOS</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>165315</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>27.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>152223</td>\n",
       "      <td>ELECTIVE</td>\n",
       "      <td>131.916667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>124321</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>162.433333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>161859</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>68.566667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>129635</td>\n",
       "      <td>EMERGENCY</td>\n",
       "      <td>84.816667</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   HADM_ID ADMISSION_TYPE         LOS\n",
       "0   165315      EMERGENCY   27.466667\n",
       "1   152223       ELECTIVE  131.916667\n",
       "2   124321      EMERGENCY  162.433333\n",
       "3   161859      EMERGENCY   68.566667\n",
       "4   129635      EMERGENCY   84.816667"
      ]
     },
     "execution_count": 514,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "admission = pd.read_csv('Dataset/temp_admission.csv')\n",
    "admission = admission[[\"HADM_ID\", \"ADMISSION_TYPE\", \"LOS\"]]\n",
    "admission['LOS'] = admission['LOS'] * 24\n",
    "admission = admission[admission['LOS'] >= hours_looks_ahead]\n",
    "\n",
    "admission.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# test_ = item220045_.loc[item220045_['HADM_ID'] == 152223].sort_values(by=['CHARTTIME'])\n",
    "# test1 = test_.groupby('HADM_ID')['CHARTTIME'].apply(list).tolist()[0]\n",
    "# test2 = test_.groupby('HADM_ID')['VALUENUM'].apply(list)\n",
    "# # for i in test1:\n",
    "# #     print(i)\n",
    "# # for j in test2:\n",
    "# #     print(j)\n",
    "# test1,test2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 392,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3548.7631926656572"
      ]
     },
     "execution_count": 392,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Heart Rate\n",
    "item220045 = pd.read_csv('Dataset/temp220045.csv')\n",
    "item220045.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item220045['CHARTTIME'] = pd.to_datetime(item220045['CHARTTIME']).dt.tz_localize(None)\n",
    "item220045 = item220045[item220045['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 371,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Arterial Blood Pressure systolic\n",
    "item220050 = pd.read_csv('Dataset/temp220050.csv')\n",
    "item220050.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'ITEM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item220050 = item220050[item220050['VALUENUM'].notna()]\n",
    "item220050['CHARTTIME'] = pd.to_datetime(item220050['CHARTTIME']).dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Non Invasive Blood Pressure systolic\n",
    "item220179 = pd.read_csv('Dataset/temp220179.csv')\n",
    "item220179.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item220179 = item220179[item220179['VALUENUM'].notna()]\n",
    "item220179['CHARTTIME'] = pd.to_datetime(item220179['CHARTTIME']).dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 373,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature Fahrenheit\n",
    "item223761 = pd.read_csv('Dataset/temp223761.csv')\n",
    "item223761.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item223761 = item223761[item223761['VALUENUM'].notna()]\n",
    "item223761['CHARTTIME'] = pd.to_datetime(item223761['CHARTTIME']).dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 374,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Temperature C\n",
    "item676 = pd.read_csv('Dataset/temp676.csv')\n",
    "item676['CHARTTIME'] = pd.to_datetime(item676['CHARTTIME']).dt.tz_localize(None)\n",
    "item676 = item676[item676['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 375,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspired O2 Fraction (FiO2)\n",
    "item223835 = pd.read_csv('Dataset/temp223835.csv')\n",
    "item223835.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'ITEM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item223835 = item223835[item223835['VALUENUM'].notna()]\n",
    "item223835['CHARTTIME'] = pd.to_datetime(item223835['CHARTTIME']).dt.tz_localize(None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 376,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RR\n",
    "item618 = pd.read_csv('Dataset/temp618.csv')\n",
    "item618['CHARTTIME'] = pd.to_datetime(item618['CHARTTIME']).dt.tz_localize(None)\n",
    "item618 = item618[item618['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 377,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RR should be merged with 618\n",
    "item220210 = pd.read_csv('Dataset/temp220210.csv')\n",
    "item220210['CHARTTIME'] = pd.to_datetime(item220210['CHARTTIME']).dt.tz_localize(None)\n",
    "item220210 = item220210[item220210['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_rr = item618.append(item220210, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 379,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spo2 \n",
    "item646 = pd.read_csv('Dataset/temp646.csv')\n",
    "item646['CHARTTIME'] = pd.to_datetime(item646['CHARTTIME']).dt.tz_localize(None)\n",
    "item646 = item646[item646['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 380,
   "metadata": {},
   "outputs": [],
   "source": [
    "# spo2 should be merged with 646\n",
    "item220277 = pd.read_csv('Dataset/temp220277.csv')\n",
    "item220277['CHARTTIME'] = pd.to_datetime(item220277['CHARTTIME']).dt.tz_localize(None)\n",
    "item220277 = item220277[item220277['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 381,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_spo2 = item646.append(item220277, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 382,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DBP\n",
    "item8441 = pd.read_csv('Dataset/temp8441.csv')\n",
    "item8441['CHARTTIME'] = pd.to_datetime(item8441['CHARTTIME']).dt.tz_localize(None)\n",
    "item8441 = item8441[item8441['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 542,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50882 serum bicarbon- ate level\n",
    "item50882 = pd.read_csv('Dataset_bench/temp50882.csv')\n",
    "item50882.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item50882['CHARTTIME'] = pd.to_datetime(item50882['CHARTTIME']).dt.tz_localize(None)\n",
    "item50882 = item50882[item50882['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 545,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50971 potassium level\n",
    "item50971 = pd.read_csv('Dataset_bench/temp50971.csv')\n",
    "item50971.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item50971['CHARTTIME'] = pd.to_datetime(item50971['CHARTTIME']).dt.tz_localize(None)\n",
    "item50971 = item50971[item50971['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 544,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50885 bilirubin level\n",
    "item50885 = pd.read_csv('Dataset_bench/temp50885.csv')\n",
    "item50885.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item50885['CHARTTIME'] = pd.to_datetime(item50885['CHARTTIME']).dt.tz_localize(None)\n",
    "item50885 = item50885[item50885['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 546,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 50983 sodium level\n",
    "item50983 = pd.read_csv('Dataset_bench/temp50983.csv')\n",
    "item50983.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item50983['CHARTTIME'] = pd.to_datetime(item50983['CHARTTIME']).dt.tz_localize(None)\n",
    "item50983 = item50983[item50983['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 547,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 51300 white blood cells count\n",
    "item51300 = pd.read_csv('Dataset_bench/temp51300.csv')\n",
    "item51300.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item51300['CHARTTIME'] = pd.to_datetime(item51300['CHARTTIME']).dt.tz_localize(None)\n",
    "item51300 = item51300[item51300['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 548,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 220739 glasgow coma scale\n",
    "item220739 = pd.read_csv('Dataset_bench/temp220739.csv')\n",
    "item220739.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item220739['CHARTTIME'] = pd.to_datetime(item220739['CHARTTIME']).dt.tz_localize(None)\n",
    "item220739 = item220739[item220739['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 837,
   "metadata": {},
   "outputs": [],
   "source": [
    "# item 50822\n",
    "item50822 = pd.read_csv('Dataset_bench/temp50822.csv')\n",
    "item50822.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item50822['CHARTTIME'] = pd.to_datetime(item50822['CHARTTIME']).dt.tz_localize(None)\n",
    "item50822 = item50822[item50822['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 550,
   "metadata": {},
   "outputs": [],
   "source": [
    "# urine output 226559 , 226560, 226561, 226563, 226564, 226565, 226567, 226584\n",
    "\n",
    "item226559 = pd.read_csv('Dataset_bench/temp226559.csv')\n",
    "item226559.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item226559['CHARTTIME'] = pd.to_datetime(item226559['CHARTTIME']).dt.tz_localize(None)\n",
    "item226559 = item226559[item226559['VALUENUM'].notna()]\n",
    "\n",
    "item226560 = pd.read_csv('Dataset_bench/temp226560.csv')\n",
    "item226560.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item226560['CHARTTIME'] = pd.to_datetime(item226560['CHARTTIME']).dt.tz_localize(None)\n",
    "item226560 = item226560[item226560['VALUENUM'].notna()]\n",
    "\n",
    "item226561 = pd.read_csv('Dataset_bench/temp226561.csv')\n",
    "item226561.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item226561['CHARTTIME'] = pd.to_datetime(item226561['CHARTTIME']).dt.tz_localize(None)\n",
    "item226561 = item226561[item226561['VALUENUM'].notna()]\n",
    "\n",
    "item226563 = pd.read_csv('Dataset_bench/temp226563.csv')\n",
    "item226563.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item226563['CHARTTIME'] = pd.to_datetime(item226563['CHARTTIME']).dt.tz_localize(None)\n",
    "item226563 = item226563[item226563['VALUENUM'].notna()]\n",
    "\n",
    "item226564 = pd.read_csv('Dataset_bench/temp226564.csv')\n",
    "item226564.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item226564['CHARTTIME'] = pd.to_datetime(item226564['CHARTTIME']).dt.tz_localize(None)\n",
    "item226564 = item226564[item226564['VALUENUM'].notna()]\n",
    "\n",
    "item226565 = pd.read_csv('Dataset_bench/temp226565.csv')\n",
    "item226565.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item226565['CHARTTIME'] = pd.to_datetime(item226565['CHARTTIME']).dt.tz_localize(None)\n",
    "item226565 = item226565[item226565['VALUENUM'].notna()]\n",
    "\n",
    "item226567 = pd.read_csv('Dataset_bench/temp226567.csv')\n",
    "item226567.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item226567['CHARTTIME'] = pd.to_datetime(item226567['CHARTTIME']).dt.tz_localize(None)\n",
    "item226567 = item226567[item226567['VALUENUM'].notna()]\n",
    "\n",
    "item226584 = pd.read_csv('Dataset_bench/temp226584.csv')\n",
    "item226584.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item226584['CHARTTIME'] = pd.to_datetime(item226584['CHARTTIME']).dt.tz_localize(None)\n",
    "item226584 = item226584[item226584['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 551,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_uo = item226559.append(item226560, ignore_index=True)\n",
    "item_uo = item_uo.append(item226561, ignore_index=True)\n",
    "item_uo = item_uo.append(item226563, ignore_index=True)\n",
    "item_uo = item_uo.append(item226564, ignore_index=True)\n",
    "item_uo = item_uo.append(item226565, ignore_index=True)\n",
    "item_uo = item_uo.append(item226567, ignore_index=True)\n",
    "item_uo = item_uo.append(item226584, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 798,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O2 50816\n",
    "item50816 = pd.read_csv('Dataset_bench/temp50816.csv')\n",
    "item50816.columns = ['ROW_ID', 'CHARTTIME', 'HADM_ID', 'SUBJECT_ID', 'VALUENUM', 'VALUEUOM']\n",
    "item50816['CHARTTIME'] = pd.to_datetime(item50816['CHARTTIME']).dt.tz_localize(None)\n",
    "item50816 = item50816[item50816['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 919,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 52 \n",
    "item52 = pd.read_csv('Dataset_bench/temp52.csv')\n",
    "item52['CHARTTIME'] = pd.to_datetime(item52['CHARTTIME']).dt.tz_localize(None)\n",
    "item52 = item52[item52['VALUENUM'].notna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 840,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge 50822 and 50971 Potassium\n",
    "item_po = item50822.append(item50971, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 920,
   "metadata": {},
   "outputs": [],
   "source": [
    "vitals_dict = {}\n",
    "np.random.seed(10)\n",
    "num_features = 1\n",
    "max_length = 2000 # length of timestamp\n",
    "gpu_num = 0\n",
    "ref_points = 192\n",
    "hid = 100\n",
    "batch = 128\n",
    "epoch = 20\n",
    "num_ids = 200\n",
    "hours_looks_ahead = 24\n",
    "num_fold = 3\n",
    "feature_set = 2\n",
    "stds = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 921,
   "metadata": {},
   "outputs": [],
   "source": [
    "# return two lists of hadm_id: list of timestamps (ascending order), list of corresponding vals of the item\n",
    "def time_val_toLst(df, hadmin_id):\n",
    "    temp = df.loc[df['HADM_ID'] == hadmin_id].sort_values(by=['CHARTTIME'])\n",
    "    if not temp.empty: \n",
    "        times = temp.groupby('HADM_ID')['CHARTTIME'].apply(list).tolist()[0]\n",
    "        vals = temp.groupby('HADM_ID')['VALUENUM'].apply(list).tolist()[0]\n",
    "        return [times, vals]\n",
    "    else:\n",
    "        return [] # if id does not have this item, return empty list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 922,
   "metadata": {},
   "outputs": [],
   "source": [
    "def flatten(df, los=hours_looks_ahead):\n",
    "    a = np.full((len(df), num_features, max_length), -100) # initlizae all as missing\n",
    "    timestamps = []\n",
    "    for i in range(len(df)):\n",
    "        l = []\n",
    "    \n",
    "        # find all the unique observed timestamps\n",
    "        for j in range(num_features):\n",
    "            ts_ij = df[i][j]\n",
    "            if ts_ij != []:\n",
    "                for ts in df[i][j][0]:\n",
    "                    if ts not in l:\n",
    "                        l.append(ts)\n",
    "        l.sort()\n",
    "        T = copy.deepcopy(l)\n",
    "        TS = []\n",
    "        for t in T:\n",
    "            if (t - T[0]).total_seconds() / 3600 <= los:\n",
    "                TS.append(t)\n",
    "        timestamps.append(TS)\n",
    "        for j in range(num_features):\n",
    "            s_ij = df[i][j]\n",
    "            if s_ij != []:\n",
    "                ts_ij = s_ij[0] \n",
    "                c_max = len(ts_ij)\n",
    "                c = 0\n",
    "                for k in range(len(TS)):\n",
    "                    if c < c_max:\n",
    "                        ts_ijc = ts_ij[c] # cur ts\n",
    "                        diff = abs(TS[k] - ts_ijc).seconds / 60 # difference between ts's\n",
    "                        if TS[k] == ts_ijc or diff < 5:\n",
    "                            a[i,j,k] = s_ij[1][c] \n",
    "                            c += 1\n",
    "    return a, timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 923,
   "metadata": {},
   "outputs": [],
   "source": [
    "def input_format(x, T):\n",
    "    real_len = 200\n",
    "    \n",
    "    for i in range(len(T)):\n",
    "        if len(T[i]) > real_len:\n",
    "            T[i] = T[i][:real_len]\n",
    "\n",
    "    x = a[:, :, :real_len]\n",
    "    M = np.zeros_like(x)\n",
    "    delta = np.zeros_like(x)\n",
    "\n",
    "    for t in T:\n",
    "        for i in range(1, len(t)):\n",
    "            t[i] = (t[i] - t[0]).total_seconds() / 3600 # hours difference\n",
    "        if len(t) != 0:\n",
    "            t[0] = 0\n",
    "    \n",
    "    # count outliers and negative values as missing values\n",
    "    # M = 0 indicates missing value\n",
    "    # M = 1 indicates observed value\n",
    "    # now since we have mask variable, we don't need -100\n",
    "    M[x > 500] = 0\n",
    "    x[x > 500] = 0.0\n",
    "    M[x < 0] = 0\n",
    "    x[x < 0] = 0.0\n",
    "    M[x > 0] = 1\n",
    "\n",
    "    for i in range(num_features):\n",
    "        for j in range(x.shape[0]):\n",
    "            for k in range(len(T[j])):\n",
    "                delta[j, i, k] = T[j][k]\n",
    "    return x, M, delta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 924,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_mean(M, x):\n",
    "    counts = np.sum(np.sum(M, axis=2), axis=0)\n",
    "    mean_values = np.sum(np.sum(x*M, axis=2), axis=0)/counts\n",
    "    for i in range(M.shape[0]):\n",
    "        for j in range(M.shape[1]):\n",
    "            if np.sum(M[i, j]) == 0:\n",
    "                M[i, j, 0] = 1\n",
    "                x[i, j, 0] = mean_values[j]\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 925,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hold_out(mask, perc=0.2):\n",
    "    \"\"\"To implement the autoencoder component of the loss, we introduce a set\n",
    "    of masking variables mr (and mr1) for each data point. If drop_mask = 0,\n",
    "    then we removecthe data point as an input to the interpolation network,\n",
    "    and includecthe predicted value at this time point when assessing\n",
    "    the autoencoder loss. In practice, we randomly select 20% of the\n",
    "    observed data points to hold out from\n",
    "    every input time series.\"\"\"\n",
    "    drop_mask = np.ones_like(mask)\n",
    "    drop_mask *= mask\n",
    "    for i in range(mask.shape[0]):\n",
    "        for j in range(mask.shape[1]):\n",
    "            count = np.sum(mask[i, j], dtype='int')\n",
    "            if int(0.20*count) > 1:\n",
    "                index = 0\n",
    "                r = np.ones((count, 1))\n",
    "                b = np.random.choice(count, int(0.20*count), replace=False)\n",
    "                r[b] = 0\n",
    "                for k in range(mask.shape[2]):\n",
    "                    if mask[i, j, k] > 0:\n",
    "                        drop_mask[i, j, k] = r[index]\n",
    "                        index += 1\n",
    "    return drop_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 926,
   "metadata": {},
   "outputs": [],
   "source": [
    "# random.seed(123)\n",
    "adm_ids = admission['HADM_ID'].tolist()\n",
    "needed_ids = random.sample(adm_ids,  num_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 927,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the vitals dictionary: {id: [item1, item2, item3, item4, item5]}\n",
    "# item1 = [list of timestamps, list of vals]\n",
    "# only use the first 200 hadm_ids as sample\n",
    "\n",
    "# 220045, spo2, rr,  \n",
    "# needed_ids = adm_ids[:num_ids]\n",
    "for adm_id in needed_ids:\n",
    "    if feature_set == 1:\n",
    "        vitals_dict[adm_id] = [time_val_toLst(item220045, adm_id)]\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item220050, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item220179, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item676, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item223835, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item_rr, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item_spo2, adm_id))\n",
    "        vitals_dict[adm_id].append(time_val_toLst(item8441, adm_id))\n",
    "        \n",
    "    if feature_set == 2:\n",
    "#         vitals_dict[adm_id] = [time_val_toLst(item676, adm_id)]\n",
    "#         vitals_dict[adm_id] = [time_val_toLst(item50816, adm_id)]\n",
    "#         vitals_dict[adm_id].append(time_val_toLst(item50882, adm_id))\n",
    "#         vitals_dict[adm_id].append(time_val_toLst(item50885, adm_id))\n",
    "        vitals_dict[adm_id] = [time_val_toLst(item52, adm_id)]\n",
    "#         vitals_dict[adm_id].append(time_val_toLst(item50971, adm_id))\n",
    "#         vitals_dict[adm_id].append(time_val_toLst(item50983, adm_id))\n",
    "#         vitals_dict[adm_id].append(time_val_toLst(item51300, adm_id))\n",
    "#         vitals_dict[adm_id].append(time_val_toLst(item220045, adm_id))\n",
    "#         vitals_dict[adm_id].append(time_val_toLst(item220179, adm_id))\n",
    "#         vitals_dict[adm_id].append(time_val_toLst(item220739, adm_id))\n",
    "#         vitals_dict[adm_id].append(time_val_toLst(item223835, adm_id))\n",
    "#         vitals_dict[adm_id].append(time_val_toLst(item_uo, adm_id))\n",
    "#         vitals_dict[adm_id] = [time_val_toLst(item_po, adm_id)]\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 928,
   "metadata": {},
   "outputs": [],
   "source": [
    "stds = []\n",
    "if feature_set == 1:\n",
    "    stds.append(item220045['VALUENUM'].std())\n",
    "    stds.append(item220050['VALUENUM'].std())\n",
    "    stds.append(item220179['VALUENUM'].std())\n",
    "    stds.append(item676['VALUENUM'].std())\n",
    "    stds.append(item223835['VALUENUM'].std())\n",
    "    stds.append(item_rr['VALUENUM'].std())\n",
    "    stds.append(item_spo2['VALUENUM'].std())\n",
    "    stds.append(item8441['VALUENUM'].std())\n",
    "if feature_set == 2:\n",
    "#     stds.append(item50816['VALUENUM'].std())\n",
    "#     stds.append(item676['VALUENUM'].std())\n",
    "#     stds.append(item50822['VALUENUM'].std())\n",
    "#     stds.append(item50882['VALUENUM'].std())\n",
    "#     stds.append(item50885['VALUENUM'].std())\n",
    "#     stds.append(item50971['VALUENUM'].std())\n",
    "#     stds.append(item50983['VALUENUM'].std())\n",
    "#     stds.append(item51300['VALUENUM'].std())\n",
    "#     stds.append(item220045['VALUENUM'].std())\n",
    "#     stds.append(item220179['VALUENUM'].std())    \n",
    "#     stds.append(item220739['VALUENUM'].std())\n",
    "#     stds.append(item223835['VALUENUM'].std())\n",
    "#     stds.append(item_uo['VALUENUM'].std()) \n",
    "#     stds.append(item_po['VALUENUM'].std()) \n",
    "    stds.append(item52['VALUENUM'].std()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 929,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def y_trans(val):\n",
    "#     if val <= 48:\n",
    "#         return 0\n",
    "#     elif 48 < val <= 168:\n",
    "#         return 1\n",
    "#     else:\n",
    "#         return 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 930,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[array([  1.04930556,  32.30618989,  89.80744949, 122.91527778])]\n"
     ]
    }
   ],
   "source": [
    "vitals = [vitals_dict[x] for x in needed_ids] # hadm_id(los>=48h): all the vitals values\n",
    "kb = KBinsDiscretizer(n_bins=3, encode='ordinal', strategy='kmeans')\n",
    "label = kb.fit_transform(admission[admission['HADM_ID'].isin(needed_ids)].LOS.to_numpy().reshape(-1,1))\n",
    "# label = admission[admission['HADM_ID'].isin(needed_ids)].LOS.tolist()\n",
    "# label = [y_trans(l) for l in label]\n",
    "print(kb.bin_edges_ / 24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 931,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(200, 4, 200) (200, 1)\n"
     ]
    }
   ],
   "source": [
    "a, ts = flatten(vitals, hours_looks_ahead)\n",
    "x, m, T = input_format(a, ts)\n",
    "missing_mean(m, x)\n",
    "X = np.concatenate((x, m, T, hold_out(m)), axis=1)  # input format\n",
    "y = np.array(label)\n",
    "print(X.shape, y.shape)\n",
    "timestamp = X.shape[2]\n",
    "num_features = X.shape[1] // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 932,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 932,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 933,
   "metadata": {},
   "outputs": [],
   "source": [
    "# [17.3, 22, 22.45, 2.32, 23.41，5.7, 3.33, 14.1]\n",
    "def customloss(ytrue, ypred):\n",
    "    \"\"\" Autoencoder loss\n",
    "    \"\"\"\n",
    "    num_fs = num_features\n",
    "    # standard deviation of each feature mentioned in paper for MIMIC_III data\n",
    "    wc = np.array(stds)\n",
    "    print(wc.shape, ytrue.shape)\n",
    "    wc.shape = (1, num_fs)\n",
    "    y = ytrue[:, :num_fs, :]\n",
    "    m2 = ytrue[:, 3*num_fs:4*num_fs, :]\n",
    "    m2 = 1 - m2\n",
    "    m1 = ytrue[:, num_fs:2*num_fs, :]\n",
    "    m = m1*m2\n",
    "    ypred = ypred[:, :num_fs, :]\n",
    "    x = (y - ypred)*(y - ypred)\n",
    "    x = x*m\n",
    "    count = tf.reduce_sum(m, axis=2)\n",
    "    count = tf.where(count > 0, count, tf.ones_like(count))\n",
    "    x = tf.reduce_sum(x, axis=2)/count\n",
    "    x = x/(wc**2)  # dividing by standard deviation\n",
    "    x = tf.reduce_sum(x, axis=1)/num_fs\n",
    "    return tf.reduce_mean(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 934,
   "metadata": {},
   "outputs": [],
   "source": [
    "def interp_net():\n",
    "    num_fs = num_features\n",
    "    if gpu_num > 1:\n",
    "        dev = \"/cpu:0\"\n",
    "    else:\n",
    "        dev = \"/gpu:0\"\n",
    "    with tf.device(dev):\n",
    "        main_input = Input(shape=(4*num_fs, timestamp), name='input')\n",
    "        sci = single_channel_interp(ref_points, hours_looks_ahead)\n",
    "        cci = cross_channel_interp()\n",
    "        interp = cci(sci(main_input))\n",
    "        reconst = cci(sci(main_input, reconstruction=True),\n",
    "                      reconstruction=True)\n",
    "        aux_output = Lambda(lambda x: x, name='aux_output')(reconst)\n",
    "        z = Permute((2, 1))(interp)\n",
    "        z = GRU(hid, activation='tanh', recurrent_dropout=0.2, dropout=0.2)(z)\n",
    "        main_output = Dense(1, activation='sigmoid', name='main_output')(z)\n",
    "        orig_model = Model([main_input], [main_output, aux_output])\n",
    "    if gpu_num > 1:\n",
    "        model = multi_gpu_model(orig_model, gpus=gpu_num)\n",
    "    else:\n",
    "        model = orig_model\n",
    "    print(orig_model.summary())\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 935,
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 0\n",
    "results = {}\n",
    "results['loss'] = []\n",
    "results['auc'] = []\n",
    "results['acc'] = []\n",
    "results['auprc'] = []\n",
    "earlystop = keras.callbacks.EarlyStopping(monitor='val_loss', min_delta=0.0000, patience=20, verbose=0)\n",
    "callbacks_list = [earlystop]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 936,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running Fold: 1\n",
      "Model: \"model_93\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 4, 200)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "single_channel_interp_93 (singl multiple             1           input[0][0]                      \n",
      "                                                                 input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "cross_channel_interp_93 (cross_ multiple             1           single_channel_interp_93[0][0]   \n",
      "                                                                 single_channel_interp_93[1][0]   \n",
      "__________________________________________________________________________________________________\n",
      "permute_93 (Permute)            (None, 192, 3)       0           cross_channel_interp_93[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "gru_93 (GRU)                    (None, 100)          31200       permute_93[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 1)            101         gru_93[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "aux_output (Lambda)             (None, 1, 200)       0           cross_channel_interp_93[1][0]    \n",
      "==================================================================================================\n",
      "Total params: 31,303\n",
      "Trainable params: 31,303\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n",
      "(1,) (?, ?, ?)\n",
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      " - 17s - loss: 62790600801386496.0000 - main_output_loss: 63424847211921408.0000 - aux_output_loss: 0.0840 - main_output_accuracy: 0.2125 - val_loss: 1.3042 - val_main_output_loss: 1.2411 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 2/20\n",
      " - 1s - loss: 48373310416748544.0000 - main_output_loss: 48861927371177984.0000 - aux_output_loss: 0.0841 - main_output_accuracy: 0.3250 - val_loss: 1.1964 - val_main_output_loss: 1.1322 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 3/20\n",
      " - 1s - loss: 34719641595019264.0000 - main_output_loss: 35070345002090496.0000 - aux_output_loss: 0.0840 - main_output_accuracy: 0.3125 - val_loss: 1.0996 - val_main_output_loss: 1.0345 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 4/20\n",
      " - 1s - loss: 23065857722679296.0000 - main_output_loss: 23298846813585408.0000 - aux_output_loss: 0.0840 - main_output_accuracy: 0.3375 - val_loss: 1.0114 - val_main_output_loss: 0.9454 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      " - 1s - loss: 11141386757734400.0000 - main_output_loss: 11253925638307840.0000 - aux_output_loss: 0.0840 - main_output_accuracy: 0.5375 - val_loss: 0.9289 - val_main_output_loss: 0.8620 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      " - 1s - loss: 4626784658128896.0000 - main_output_loss: 4673519807889408.0000 - aux_output_loss: 0.0840 - main_output_accuracy: 0.7750 - val_loss: 0.8571 - val_main_output_loss: 0.7895 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      " - 1s - loss: 5681922831286272.0000 - main_output_loss: 5739315942391808.0000 - aux_output_loss: 0.0840 - main_output_accuracy: 0.8375 - val_loss: 0.7958 - val_main_output_loss: 0.7276 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 0.0500\n",
      "Epoch 8/20\n",
      " - 1s - loss: 3764206243414016.0000 - main_output_loss: 3802228515143680.0000 - aux_output_loss: 0.0840 - main_output_accuracy: 0.9125 - val_loss: 0.7443 - val_main_output_loss: 0.6755 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 0.9000\n",
      "Epoch 9/20\n",
      " - 1s - loss: 7491917612843008.0000 - main_output_loss: 7567593325985792.0000 - aux_output_loss: 0.0840 - main_output_accuracy: 0.8875 - val_loss: 0.6999 - val_main_output_loss: 0.6307 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 0.9000\n",
      "Epoch 10/20\n",
      " - 1s - loss: 6666079153684480.0000 - main_output_loss: 6733412966596608.0000 - aux_output_loss: 0.0841 - main_output_accuracy: 0.9125 - val_loss: 0.6631 - val_main_output_loss: 0.5935 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 0.9500\n",
      "Epoch 11/20\n",
      " - 1s - loss: 9853634229567488.0000 - main_output_loss: 9953165801684992.0000 - aux_output_loss: 0.0841 - main_output_accuracy: 0.9000 - val_loss: 0.6311 - val_main_output_loss: 0.5612 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 0.9500\n",
      "Epoch 12/20\n",
      " - 1s - loss: 3461532113436672.0000 - main_output_loss: 3496497173757952.0000 - aux_output_loss: 0.0841 - main_output_accuracy: 0.9250 - val_loss: 0.6055 - val_main_output_loss: 0.5353 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 0.9500\n",
      "Epoch 13/20\n",
      " - 1s - loss: 5837332464795648.0000 - main_output_loss: 5896295386447872.0000 - aux_output_loss: 0.0841 - main_output_accuracy: 0.9250 - val_loss: 0.5826 - val_main_output_loss: 0.5122 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 0.9500\n",
      "Epoch 14/20\n",
      " - 1s - loss: 7229963799363584.0000 - main_output_loss: 7302993812652032.0000 - aux_output_loss: 0.0841 - main_output_accuracy: 0.9250 - val_loss: 0.5619 - val_main_output_loss: 0.4913 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 0.9500\n",
      "Epoch 15/20\n",
      " - 1s - loss: 6265547683528704.0000 - main_output_loss: 6328836174118912.0000 - aux_output_loss: 0.0841 - main_output_accuracy: 0.9250 - val_loss: 0.5438 - val_main_output_loss: 0.4730 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 0.9500\n",
      "Epoch 16/20\n",
      " - 1s - loss: 7361493951578112.0000 - main_output_loss: 7435852183502848.0000 - aux_output_loss: 0.0841 - main_output_accuracy: 0.9250 - val_loss: 0.5286 - val_main_output_loss: 0.4576 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 1.0000\n",
      "Epoch 17/20\n",
      " - 1s - loss: 7144438652469248.0000 - main_output_loss: 7216604840460288.0000 - aux_output_loss: 0.0841 - main_output_accuracy: 0.9000 - val_loss: 0.5176 - val_main_output_loss: 0.4465 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 1.0000\n",
      "Epoch 18/20\n",
      " - 1s - loss: 4983727042068480.0000 - main_output_loss: 5034067816873984.0000 - aux_output_loss: 0.0841 - main_output_accuracy: 0.9125 - val_loss: 0.5076 - val_main_output_loss: 0.4364 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 1.0000\n",
      "Epoch 19/20\n",
      " - 1s - loss: 6202566819971072.0000 - main_output_loss: 6265219118530560.0000 - aux_output_loss: 0.0841 - main_output_accuracy: 0.9125 - val_loss: 0.4988 - val_main_output_loss: 0.4275 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 1.0000\n",
      "Epoch 20/20\n",
      " - 1s - loss: 5363474628608000.0000 - main_output_loss: 5417651346079744.0000 - aux_output_loss: 0.0841 - main_output_accuracy: 0.9250 - val_loss: 0.4909 - val_main_output_loss: 0.4196 - val_aux_output_loss: 0.0763 - val_main_output_accuracy: 1.0000\n",
      "{'loss': [0.4442867338657379], 'auc': [], 'acc': [0.949999988079071], 'auprc': [0.2409560723514212]}\n",
      "Running Fold: 2\n",
      "Model: \"model_94\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input (InputLayer)              (None, 4, 200)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "single_channel_interp_94 (singl multiple             1           input[0][0]                      \n",
      "                                                                 input[0][0]                      \n",
      "__________________________________________________________________________________________________\n",
      "cross_channel_interp_94 (cross_ multiple             1           single_channel_interp_94[0][0]   \n",
      "                                                                 single_channel_interp_94[1][0]   \n",
      "__________________________________________________________________________________________________\n",
      "permute_94 (Permute)            (None, 192, 3)       0           cross_channel_interp_94[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "gru_94 (GRU)                    (None, 100)          31200       permute_94[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "main_output (Dense)             (None, 1)            101         gru_94[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "aux_output (Lambda)             (None, 1, 200)       0           cross_channel_interp_94[1][0]    \n",
      "==================================================================================================\n",
      "Total params: 31,303\n",
      "Trainable params: 31,303\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1,) (?, ?, ?)\n",
      "Train on 80 samples, validate on 20 samples\n",
      "Epoch 1/20\n",
      " - 17s - loss: 6532043961794560.0000 - main_output_loss: 6598024323137536.0000 - aux_output_loss: 0.0591 - main_output_accuracy: 0.8500 - val_loss: 0.3372 - val_main_output_loss: 0.3018 - val_aux_output_loss: 0.0388 - val_main_output_accuracy: 1.0000\n",
      "Epoch 2/20\n",
      " - 1s - loss: 7214964699824128.0000 - main_output_loss: 7287843315515392.0000 - aux_output_loss: 0.0592 - main_output_accuracy: 0.7375 - val_loss: 0.3295 - val_main_output_loss: 0.2940 - val_aux_output_loss: 0.0388 - val_main_output_accuracy: 1.0000\n",
      "Epoch 3/20\n",
      " - 1s - loss: 9702528690159616.0000 - main_output_loss: 9800533401403392.0000 - aux_output_loss: 0.0592 - main_output_accuracy: 0.8750 - val_loss: 0.3242 - val_main_output_loss: 0.2887 - val_aux_output_loss: 0.0388 - val_main_output_accuracy: 1.0000\n",
      "Epoch 4/20\n",
      " - 1s - loss: 3752466755616768.0000 - main_output_loss: 3790370378874880.0000 - aux_output_loss: 0.0593 - main_output_accuracy: 0.8375 - val_loss: nan - val_main_output_loss: nan - val_aux_output_loss: 0.0388 - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 5/20\n",
      " - 1s - loss: nan - main_output_loss: nan - aux_output_loss: 0.0593 - main_output_accuracy: 0.0000e+00 - val_loss: nan - val_main_output_loss: nan - val_aux_output_loss: nan - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 6/20\n",
      " - 1s - loss: nan - main_output_loss: nan - aux_output_loss: nan - main_output_accuracy: 0.0000e+00 - val_loss: nan - val_main_output_loss: nan - val_aux_output_loss: nan - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 7/20\n",
      " - 1s - loss: nan - main_output_loss: nan - aux_output_loss: nan - main_output_accuracy: 0.0000e+00 - val_loss: nan - val_main_output_loss: nan - val_aux_output_loss: nan - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 8/20\n",
      " - 1s - loss: nan - main_output_loss: nan - aux_output_loss: nan - main_output_accuracy: 0.0000e+00 - val_loss: nan - val_main_output_loss: nan - val_aux_output_loss: nan - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 9/20\n",
      " - 1s - loss: nan - main_output_loss: nan - aux_output_loss: nan - main_output_accuracy: 0.0000e+00 - val_loss: nan - val_main_output_loss: nan - val_aux_output_loss: nan - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 10/20\n",
      " - 1s - loss: nan - main_output_loss: nan - aux_output_loss: nan - main_output_accuracy: 0.0000e+00 - val_loss: nan - val_main_output_loss: nan - val_aux_output_loss: nan - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 11/20\n",
      " - 1s - loss: nan - main_output_loss: nan - aux_output_loss: nan - main_output_accuracy: 0.0000e+00 - val_loss: nan - val_main_output_loss: nan - val_aux_output_loss: nan - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 12/20\n",
      " - 1s - loss: nan - main_output_loss: nan - aux_output_loss: nan - main_output_accuracy: 0.0000e+00 - val_loss: nan - val_main_output_loss: nan - val_aux_output_loss: nan - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 13/20\n",
      " - 1s - loss: nan - main_output_loss: nan - aux_output_loss: nan - main_output_accuracy: 0.0000e+00 - val_loss: nan - val_main_output_loss: nan - val_aux_output_loss: nan - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 14/20\n",
      " - 1s - loss: nan - main_output_loss: nan - aux_output_loss: nan - main_output_accuracy: 0.0000e+00 - val_loss: nan - val_main_output_loss: nan - val_aux_output_loss: nan - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 15/20\n",
      " - 1s - loss: nan - main_output_loss: nan - aux_output_loss: nan - main_output_accuracy: 0.0000e+00 - val_loss: nan - val_main_output_loss: nan - val_aux_output_loss: nan - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 16/20\n",
      " - 1s - loss: nan - main_output_loss: nan - aux_output_loss: nan - main_output_accuracy: 0.0000e+00 - val_loss: nan - val_main_output_loss: nan - val_aux_output_loss: nan - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 17/20\n",
      " - 1s - loss: nan - main_output_loss: nan - aux_output_loss: nan - main_output_accuracy: 0.0000e+00 - val_loss: nan - val_main_output_loss: nan - val_aux_output_loss: nan - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 18/20\n",
      " - 1s - loss: nan - main_output_loss: nan - aux_output_loss: nan - main_output_accuracy: 0.0000e+00 - val_loss: nan - val_main_output_loss: nan - val_aux_output_loss: nan - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 19/20\n",
      " - 1s - loss: nan - main_output_loss: nan - aux_output_loss: nan - main_output_accuracy: 0.0000e+00 - val_loss: nan - val_main_output_loss: nan - val_aux_output_loss: nan - val_main_output_accuracy: 0.0000e+00\n",
      "Epoch 20/20\n",
      " - 1s - loss: nan - main_output_loss: nan - aux_output_loss: nan - main_output_accuracy: 0.0000e+00 - val_loss: nan - val_main_output_loss: nan - val_aux_output_loss: nan - val_main_output_accuracy: 0.0000e+00\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "multiclass format is not supported",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-936-cada76003448>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     27\u001b[0m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'acc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;31m#     results['auc'].append(auc_score(y[test], y_pred))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 29\u001b[0;31m     \u001b[0mresults\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'auprc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauprc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     30\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     31\u001b[0m     \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_ranking.py\u001b[0m in \u001b[0;36maverage_precision_score\u001b[0;34m(y_true, y_score, average, pos_label, sample_weight)\u001b[0m\n\u001b[1;32m    213\u001b[0m                                 pos_label=pos_label)\n\u001b[1;32m    214\u001b[0m     return _average_binary_score(average_precision, y_true, y_score,\n\u001b[0;32m--> 215\u001b[0;31m                                  average, sample_weight=sample_weight)\n\u001b[0m\u001b[1;32m    216\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_base.py\u001b[0m in \u001b[0;36m_average_binary_score\u001b[0;34m(binary_metric, y_true, y_score, average, sample_weight)\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0my_type\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"multilabel-indicator\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"{0} format is not supported\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"binary\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: multiclass format is not supported"
     ]
    }
   ],
   "source": [
    "# 5-fold cross-validation\n",
    "i = 0\n",
    "kfold = StratifiedKFold(n_splits=2, shuffle=True, random_state=seed)\n",
    "for train, test in kfold.split(np.zeros(len(y)), y):\n",
    "    print(\"Running Fold:\", i+1)\n",
    "    model = interp_net()  # re-initializing every time\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss={'main_output': 'binary_crossentropy', 'aux_output': customloss},\n",
    "        loss_weights={'main_output': 0.99, 'aux_output': 0.99},\n",
    "        metrics={'main_output': 'accuracy'})\n",
    "    model.fit(\n",
    "        {'input': X[train]}, {'main_output': y[train], 'aux_output': X[train]},\n",
    "        batch_size=batch,\n",
    "        callbacks=callbacks_list,\n",
    "        nb_epoch=epoch,\n",
    "        validation_split=0.20,\n",
    "        verbose=2)\n",
    "    y_pred = model.predict(X[test], batch_size=batch)\n",
    "    y_pred = y_pred[0]\n",
    "    total_loss, score, reconst_loss, acc = model.evaluate(\n",
    "        {'input': X[test]},\n",
    "        {'main_output': y[test], 'aux_output': X[test]},\n",
    "        batch_size=batch,\n",
    "        verbose=0)\n",
    "    results['loss'].append(score)\n",
    "    results['acc'].append(acc)\n",
    "#     results['auc'].append(auc_score(y[test], y_pred))\n",
    "    results['auprc'].append(auprc(y[test], y_pred))\n",
    "    print(results)\n",
    "    i += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 937,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': [0.4442867338657379, nan], 'auc': [], 'acc': [0.949999988079071, 0.0], 'auprc': [0.2409560723514212]}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 665,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.8490171432495117,\n",
       "  0.8478379249572754,\n",
       "  0.5991557240486145,\n",
       "  0.20699283480644226,\n",
       "  1.7641651630401611],\n",
       " 'auc': [0.44594594594594594,\n",
       "  0.4864864864864865,\n",
       "  0.5263157894736843,\n",
       "  0.5263157894736842,\n",
       "  0.5789473684210527],\n",
       " 'acc': [0.07500000298023224,\n",
       "  0.07500000298023224,\n",
       "  0.949999988079071,\n",
       "  0.949999988079071,\n",
       "  0.05000000074505806],\n",
       " 'auprc': [0.075,\n",
       "  0.07894736842105263,\n",
       "  0.05263157894736842,\n",
       "  0.05555555555555555,\n",
       "  0.058823529411764705]}"
      ]
     },
     "execution_count": 665,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "{'loss': [0.4442867338657379, nan], 'auc': [], 'acc': [0.949999988079071, 0.0], 'auprc': [0.2409560723514212]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {},
   "outputs": [],
   "source": [
    "# {'loss': [0.7575289607048035, 0.30154579877853394, 0.14339998364448547, nan], 'auc': [], 'acc': [0.42500001192092896, 0.9750000238418579, 0.9750000238418579, 0.0], 'auprc': [0.06666666666666667, 0.3333333333333333, 0.09090909090909091]}\n",
    "# for lst in timestamps:\n",
    "#     if len(lst) > max_length:\n",
    "#         timestamps[i] = lst[:max_length]\n",
    "\n",
    "# x = a[:, :, :max_length]\n",
    "# M = np.zeros_like(x)\n",
    "# delta = np.zeros_like(x)\n",
    "\n",
    "# for t in timestamps:\n",
    "#     for i in range(1, len(t)):\n",
    "#         t[i] = (t[i] - t[0]).total_seconds() / 3600 # hours difference\n",
    "#     if len(t) != 0:\n",
    "#         t[0] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a = np.full((len(vitals), num_features, max_length), -100) # initlizae all as missing\n",
    "# timestamps = []\n",
    "# for i in range(len(vitals)):\n",
    "#     l = []\n",
    "    \n",
    "#     # find all the unique observed timestamps\n",
    "#     for j in range(num_features):\n",
    "#         ts_ij = vitals[i][j]\n",
    "#         if ts_ij != []:\n",
    "#             for ts in vitals[i][j][0]:\n",
    "#                 if ts not in l:\n",
    "#                     l.append(ts)\n",
    "#     l.sort()\n",
    "#     T = copy.deepcopy(l)\n",
    "#     TS = []\n",
    "#     for t in T:\n",
    "#         if (t - T[0]).total_seconds() / 3600 <= 72:\n",
    "#             TS.append(t)\n",
    "#     timestamps.append(TS)\n",
    "#     for j in range(num_features):\n",
    "#         s_ij = vitals[i][j]\n",
    "#         if s_ij != []:\n",
    "#             ts_ij = s_ij[0] \n",
    "#             c_max = len(ts_ij)\n",
    "#             c = 0\n",
    "#             for k in range(len(TS)):\n",
    "#                 if c < c_max:\n",
    "#                     ts_ijc = ts_ij[c] # cur ts\n",
    "#                     diff = abs(TS[k] - ts_ijc).seconds / 60 # difference between ts's\n",
    "#                     if TS[k] == ts_ijc or diff < 5:\n",
    "#                         a[i,j,k] = s_ij[1][c] \n",
    "#                         c += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # count outliers and negative values as missing values\n",
    "# # M = 0 indicates missing value\n",
    "# # M = 1 indicates observed value\n",
    "# # now since we have mask variable, we don't need -100\n",
    "# M[x > 500] = 0\n",
    "# x[x > 500] = 0.0\n",
    "# M[x < 0] = 0\n",
    "# x[x < 0] = 0.0\n",
    "# M[x > 0] = 1\n",
    "\n",
    "# for i in range(num_features):\n",
    "#         for j in range(x.shape[0]):\n",
    "#             for k in range(len(timestamps[j])):\n",
    "#                 delta[j, i, k] = timestamps[j][k]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[39, 38, 13, 33, 22, 2, 37, 21, 29, 32, 27, 31, 53, 35, 47, 30, 31, 44, 25, 28]"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = np.concatenate((x, m, T, hold_out(m)), axis=1)  # input format\n",
    "y = np.array(label)\n",
    "print(x.shape, y.shape)\n",
    "timestamp = x.shape[2]\n",
    "num_features = x.shape[1] // 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 429,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'loss': [0.18130822479724884, nan],\n",
       " 'auc': [0.6025641025641025],\n",
       " 'acc': [0.9750000238418579, 0.0],\n",
       " 'auprc': [0.04805491990846682]}"
      ]
     },
     "execution_count": 429,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
